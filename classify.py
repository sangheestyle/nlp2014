from collections import defaultdict
from csv import DictReader, DictWriter

import os
import operator
import nltk
from nltk.corpus import stopwords
from nltk.util import *
from nltk.corpus import wordnet as wn
from nltk.tokenize import TreebankWordTokenizer
from nltk.corpus.reader.conll import ConllCorpusReader

kTOKENIZER = TreebankWordTokenizer()
TOPGUESSES = 40
PUNC = ['-',',','.',':',';','\'','\"','?','!']
COLUMN_TYPES = ('words', 'pos', 'tree', 'chunk', 'ne', 'srl', 'ignore')


def morphy_stem(word):
    """
    Simple stemmer
    """
    stem = wn.morphy(word)
    if stem:
        return stem.lower()
    else:
        return word.lower()

def compose_top_scores(qt_dict, ir_dict, n):
    """
        Given two sets of guesses, this will return a dictionaty containing
        the n top guesses of the two models
    """
    h = n - (n/2)
    top_guesses = {}
    for i in xrange(h):
        top_guesses[qt_dict[i][0]] = qt_dict[i][1]
    i = 0
    c = 0
    
    while (i+h) < n:
        while True:
            if ir_dict[c][0] not in top_guesses:
                break
            c += 1
        top_guesses[ir_dict[c][0]] = ir_dict[c][1]
        i += 1

    return top_guesses

def get_answer(qt_dict, ir_dict, n, rank):
    """
        This will return the answer ranked "rank" among the top guesses generated by compose_top_scores
    """
    """
    if rank < 0:
        #print "#"
        return qt_dict[0][0]
    """
    h = n - (n/2)
    if rank < h:
        return qt_dict[rank][0]
    else:
        return ir_dict[rank-h][0]

def form_dict(vals):
    d = defaultdict(float)
    for jj in vals.split(", "):
        key, val = jj.split(":")
        d[key.strip()] = float(val)
    return d


class FeatureExtractor:
    def __init__(self):
        """
        You may want to add code here
        """
        
        None
    
    def features(self, dict):
        d = defaultdict(int)
        """
        for ii in kTOKENIZER.tokenize(dict['Question Text']):
            d[morphy_stem(ii)] += 1
        """
        qd = form_dict(dict['QANTA Scores'])
        wd = form_dict(dict['IR_Wiki Scores'])
        sp = int(dict['Sentence Position'])
        qtext = dict['Question Text']
        list_words = []
        
        sorted_qd = sorted(qd.items(), key=operator.itemgetter(1), reverse=True)
        sorted_wd = sorted(wd.items(), key=operator.itemgetter(1), reverse=True)
        overlap = 0
        consider_qanta = 0
        """
        if sorted_qd[0][0] == sorted_wd[0][0] and sorted_qd[0][1] > 0.5:
            consider_qanta = 1
        """
        
        for w in kTOKENIZER.tokenize(qtext):
            if consider_qanta != 0:
                break
            if morphy_stem(w) not in stopwords.words('english') and w[0] not in PUNC:
                list_words.append(morphy_stem(w))
        #bigrams_qtext = list(bigrams(list_words))
        trigrams_qtext = list(trigrams(list_words))
        highest = 0
        rnk = 0
        
        for i in xrange(10):
            if consider_qanta != 0:
                break
            fid = "./wikipedia/"+sorted_qd[i][0][0]+"/"+sorted_qd[i][0]
            v = 0
            if os.path.exists(fid):
                fids = [fid]
                cr = ConllCorpusReader("", fids, COLUMN_TYPES)
                c = 0
                for r in cr.iob_sents():
                    if c > 3:
                        break
                    wiki_words = []
                    for j in xrange(len(r)):
                        w = morphy_stem(r[j][1].lower())
                        if w not in stopwords.words('english') and w[0] not in PUNC:
                            wiki_words.append(w)
                
                    for j in xrange(len(wiki_words)-2):
                        bgrm = (wiki_words[j],wiki_words[j+1],wiki_words[j+2])
                        if bgrm in trigrams_qtext:
                            ky = "Bigrams"+str(i)
                            d[ky] += TOPGUESSES - i
                            v += 1
                            print "QID:",dict['Question ID'],"SP=",sp,"\t", rnk,v, bgrm[0],bgrm[1],bgrm[2]
                    c += 1
            if v > highest:
                highest = v
                rnk = i
        d['bigrams'] = rnk
        """
        highest = 0
        rnk = 0
        qwords = kTOKENIZER.tokenize(qtext)
        for i in xrange(10):
            fid = "./wikipedia/"+sorted_qd[i][0][0]+"/"+sorted_qd[i][0]
            if os.path.exists(fid):
                fids = [fid]
                cr = ConllCorpusReader("", fids, COLUMN_TYPES)
                
                c = 0
                v = 0
                for r in cr.iob_sents():
                    if c > 4:
                        break
                    for j in xrange(len(r)):
                        if r[j][1].lower() not in stopwords.words('english') and r[j][1][0] not in PUNC and r[j][1].lower() in qwords:
                            v += 1
                    
                    c += 1
                if v > highest:
                    highest = v
                    rnk = i
        d['unigram'] = rnk
        """

        for qg in xrange(len(sorted_qd)):
            if sorted_qd[qg][0] == sorted_wd[0][0]:
                overlap = 1
                d['Top IR Overlap'] += qg
                """
                if  (sorted_qd[0][1] - sorted_qd[1][1]) < 0.15:
                    d['Top IR Overlap'] += qg
                else:
                    d['Top IR Overlap'] -= qg
                """
                break
                
        if overlap == 0:
            if sorted_qd[0][1] < 0.05 and sorted_wd[0][1] > 4.0:
                d['Top IR'] += 1
            if sp == 0 and sorted_qd[0][1] < 0.1:
                d['Top_IR'] += int(sorted_wd[0][1])

        for i in xrange(4*sp):
            if i < len(sorted_qd) and sorted_qd[i][0] == sorted_wd[i][0]:
                d['Equal Rank'] += i + 2
        if sorted_qd[0][1] > 0.75:
            d['Q Score'] = 0
        elif sorted_qd[0][1] > 0.39:
            d['Q Score'] = 1
        elif sorted_qd[0][1] > 0.24:
            d['Q Score'] = 2
        else:
            d['Q Score'] = -1
        if sorted_wd[0][1] > 20.0 and sorted_qd[0][1] < 0.25:
            d['IR Score'] = TOPGUESSES
        
        d['Sentence Position'] = sp
        
        return d

if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description='Process some integers.')
    parser.add_argument('--subsample', type=float, default=1.0,
                        help='subsample this amount')
    args = parser.parse_args()
    
    # Create feature extractor (you may want to modify this)
    fe = FeatureExtractor()
    
    # Read in training data
    train = DictReader(open("new_train_"+str(TOPGUESSES)+".csv", 'r'))
    
    # Split off dev section
    dev_train = []
    dev_test = []
    train_set = []
    all_set = []
    for ii in train:
        if args.subsample < 1.0 and int(ii['Question ID']) % 100 > 100 * args.subsample:
            continue

        dict = {}
        dict['Question Text'] = ii['Question Text']
        dict['Question ID'] = ii['Question ID']
        dict['QANTA Scores'] = ii['QANTA Scores']
        dict['IR_Wiki Scores'] = ii['IR_Wiki Scores']
        dict['Sentence Position'] = ii['Sentence Position']
        dict['category'] = ii['category']
        feat = fe.features(dict)
        dict['Answer Rank'] = ii['Answer Rank']
        dict['Answer'] = ii['Answer']
        train_set.append((feat, ii['Answer Rank']))
        if int(ii['Question ID']) % 5 == 0:
            
            dev_test.append((feat, ii['Answer Rank'], ii['Question ID'], ii['Sentence Position']))
            all_set.append((dict))
                
        else:
            dev_train.append((feat, ii['Answer Rank']))
        

    # Train a classifier
    print("Training classifier ...")
    classifier = nltk.classify.NaiveBayesClassifier.train(dev_train)
    #classifier = nltk.classify.MaxentClassifier.train(dev_train, 'IIS', trace=3, max_iter=100)
    columns = ['Question ID', 'Question Text', 'Sentence Position','Correct Answer', 'Our Guess', 'QANTA Scores', 'IR_Wiki Scores', 'category']
    """
    right = 0
    total = len(dev_test)

    for ii in dev_test:
        prediction = classifier.classify(ii[0])
        if prediction == ii[1]:
            right += 1

    print("Accuracy on dev: %f" % (float(right) / float(total)))
    """
    right = 0
    total = len(all_set)
    correct = DictWriter(open('correct_pred.csv', 'w'), ['Question ID','Question Text','Sentence Position','Answer','Answer Rank','Prediction','QANTA','IR_Wiki','category'])
    incorrect = DictWriter(open('incorrect_pred.csv', 'w'), ['Question ID','Question Text','Sentence Position','Answer','Answer Rank','Prediction','QANTA','IR_Wiki','category'])
    incorrect.writeheader()
    correct.writeheader()
    for ii in all_set:
        dict = {}
        dict['Question Text'] = ii['Question Text']
        dict['Question ID'] = ii['Question ID']
        dict['QANTA Scores'] = ii['QANTA Scores']
        dict['IR_Wiki Scores'] = ii['IR_Wiki Scores']
        dict['Sentence Position'] = ii['Sentence Position']
        dict['category'] = ii['category']
        feat = fe.features(dict)
        dict['Answer'] = ii['Answer']
        dict['Answer Rank'] = ii['Answer Rank']
        prediction = classifier.classify(feat)
        qd = form_dict(dict['QANTA Scores'])
        wd = form_dict(dict['IR_Wiki Scores'])
        sorted_qd = sorted(qd.items(), key=operator.itemgetter(1), reverse=True)
        sorted_wd = sorted(wd.items(), key=operator.itemgetter(1), reverse=True)

        if prediction == ii['Answer Rank']:
            right += 1
            correct.writerow({'Question ID':dict['Question ID'],'Question Text':dict['Question Text'],'Sentence Position':dict['Sentence Position'],'Answer':dict['Answer'],'Answer Rank':dict['Answer Rank'],'Prediction':prediction,'QANTA':sorted_qd,'IR_Wiki':sorted_wd,'category':dict['category']})
        else:
            incorrect.writerow({'Question ID':dict['Question ID'],'Question Text':dict['Question Text'],'Sentence Position':dict['Sentence Position'],'Answer':dict['Answer'],'Answer Rank':dict['Answer Rank'],'Prediction':prediction,'QANTA':sorted_qd,'IR_Wiki':sorted_wd,'category':dict['category']})
    print("Accuracy on dev: %f" % (float(right) / float(total)))
    print "\nLength of dev set:\t",total,"\nIncorrect pred:\t\t",(total - right)

    # Retrain on all data
    classifier = nltk.classify.NaiveBayesClassifier.train(train_set)
    #classifier = nltk.classify.MaxentClassifier.train(dev_train+ dev_test, 'IIS', trace=3, max_iter=1000)
    # Read in test section
    test = {}
    for ii in DictReader(open("test.csv")):
        dict = {}
        dict['Question Text'] = ii['Question Text']
        dict['Question ID'] = ii['Question ID']
        dict['QANTA Scores'] = ii['QANTA Scores']
        dict['IR_Wiki Scores'] = ii['IR_Wiki Scores']
        dict['Sentence Position'] = ii['Sentence Position']
        dict['category'] = ii['category']
        qd = form_dict(dict['QANTA Scores'])
        wd = form_dict(dict['IR_Wiki Scores'])
        sorted_qd = sorted(qd.items(), key=operator.itemgetter(1), reverse=True)
        sorted_wd = sorted(wd.items(), key=operator.itemgetter(1), reverse=True)
        rank = int(classifier.classify(fe.features(dict)))
        #print rank
        #test[ii['Question ID']] = get_answer(sorted_qd, sorted_qd, TOPGUESSES, rank)
        #print rank
        if rank < 20:
            if rank < 0:
                rank = 0
            test[ii['Question ID']] = sorted_qd[rank][0]
        else:
            test[ii['Question ID']] = sorted_wd[rank-TOPGUESSES][0]
        

    # Write predictions
    o = DictWriter(open('pred.csv', 'w'), ['Question ID', 'Answer'])
    o.writeheader()
    for ii in sorted(test):
        o.writerow({'Question ID': ii, 'Answer': test[ii]})







